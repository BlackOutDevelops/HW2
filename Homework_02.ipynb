{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Homework\n",
    "\n",
    "This is the 2nd assignment for CAP 4630 and we will implement logistic regression and apply it to two\n",
    "different datasets. \\\n",
    "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 100 Points)** \\\n",
    "You are **not** allowed to use Machine Learning libaries such as Scikit-learn and Keras.\n",
    "\n",
    "**Task Overview:**\n",
    "- Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Logistic Regression ##\n",
    "### 1.1 Packages\n",
    "\n",
    "Import useful packages for scientific computing and data processing. \n",
    "\n",
    "**Tasks:**\n",
    "1. Import numpy and rename it to np.\n",
    "2. Import pandas and rename it to pd.\n",
    "3. Import the pyplot function in the libraray of matplotlib and rename it to plt.\n",
    "\n",
    "References:\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "\n",
    "**Attention:**\n",
    "1. After this renaming, you will use the new name to call functions. For example, **numpy** will become **np** in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and rename libraries here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 - Data Preparation ##\n",
    "\n",
    "Prepare the data for regression task. **(20 Points)**\n",
    "\n",
    "**Tasks:**\n",
    "1. Load data for logistic regression.\n",
    "2. **Generate the SCATTER PLOT of the data**.\n",
    "\n",
    "**Hints:**\n",
    "1. The data file is \"data_logistic.csv\", which are exam scores for students.\n",
    "2. The data is organized by column: x1 (exam 1 score), x2 (exam 2 score), and label y (pass 1 or fail 0).\n",
    "3. Please use different colors for postive(label=1) and negative(label=0) data.\n",
    "4. An example of scatter plots is shown below.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1CPv5s4W8SkUMa_sXCIz-NejSnFj-e1IH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input data and generate plots\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Sigmoid function ##\n",
    "\n",
    "\n",
    "Implement sigmoid function so it can be called by the rest of your program. **(20 Points)**\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement the sigmoid function (**def sigmoid(z):**). \n",
    "2. Test the sigmoid function by function **plotting** with test data (X, Y) where Y = sigmoid(X). \n",
    "\n",
    "**Hints:**  \n",
    "1. Given the class material, sigmoid function is defined as:\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$.\n",
    "2. You may consider X = np.linspace(-5, 5, 1000) to plot the curve.\n",
    "3. Plot Y against X.\n",
    "4. An example of plot for validation is shown below:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=18j5oHdw78uVm2WwHsdIb4hwhpXDxR37S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement sigmoid fuction here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Cost function and gradient ##\n",
    "\n",
    "Implement the cross entropy cost function and its gradient for logistic regression. **(30 Points)**\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement the \"cal_cost\" to compute the cost.\n",
    "2. Implement the \"cal_grad\" to compute the gradients.\n",
    "3. Test \"cal_cost\" and \"cal_grad\" with initial values and print out the results.\n",
    "\n",
    "**Hint:**\n",
    "1. The cross entropy cost function (J(θ)) in logistic regression is shown below. It involves two terms, including ylog(h) and (1-y)log(1-h) where h is the function of x.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xLhlPFI4wekwuA7lFm7ebRVt0XBZk3e7)\n",
    "\n",
    "2. The gradient of the cost J(θ) is a vector of the same length as θ where the $j$th element (for $j = 0, 1, . . . , n)$ is defined below. You may do a hand calculation to justify the first order derivative with the term above.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xfA0A0xyRv2L5JZIdedAmEZxZ3DwpOCF)\n",
    "\n",
    "3. When you implement J(θ), please use eps = 1e-15 to prevent possible \"divide by 0 exception\" in second term. You may think about the reason.\n",
    "4. You may consider the below templates for two functions:\n",
    "\n",
    "    def cal_cost(theta, X, y):\n",
    " \n",
    "        htheta = ...\n",
    "        term1 = ...  /* matrix_multiplication(log(htheta), y)\n",
    "        term2 = ...  /* matrix_multiplication(log(1-htheta+eps), (1-y))\n",
    "        J = - 1 / m * (term1 + term2) \n",
    "        \n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    def cal_grad(theta, X, y):\n",
    "        \n",
    "        htheta = ...\n",
    "        term1 = ... /* matrix_multiplication(transpose(X), (htheta - y))  //you may think about why transpose(x)\n",
    "        grad = 1 / m * term1 \n",
    "    \n",
    "        return grad\n",
    "5. It involves matrix multiplication and you may consider the function of np.matmul or np.dot.   \n",
    "        \n",
    "6. Initialize the intercept term (constant term) with **ones** and the theta with **zeros**. Test the functions with these initial values. \\\n",
    "    **Expected outputs:**\\\n",
    "    Cost at initial theta : 0.6931471805599445\\\n",
    "    Gradient at inital theta : [-0.1        -10.91242026 -11.73652937]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the cost function here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Train parameters with Gradient Descent ##\n",
    "\n",
    "\n",
    "Train parameters using Gradient Descent. **(15 Points)**\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate best fit theta by Gradient Descent with learning rate of **0.001 (1e-3)** and epoch of **80K**. The initial theta from above blocks is used as initial values.\n",
    "2. Print out the best theta (the last one is considered as the best here) and its corresponding cost.\n",
    "3. **Plot the decision boundary**.\n",
    "\n",
    "**Hints:**\n",
    "1. You may take gradient descent in homework 1 as an template.\n",
    "2. Derive the boundary line from **sigmoid(theta[0]+ X1 * theta[1] + X2* theta[2])=0.5**. Think about why we get the line by setting **the activated probability to 0.5**. Also, try to calculate the final relationship between X1 and X2. When sigmoid(X) = 0.5, what is the value of x? Check the generated plot in 1.3.\n",
    "3. The validation of first 5 epochs (updated theta and cost): \\\n",
    "------Epoch 0------\\\n",
    "Theta: [0.0001     0.01091242 0.01173653]\\\n",
    "Cost: 0.6996118077359638\\\n",
    "------Epoch 1------\\\n",
    "Theta: [-0.0001129   0.00053949  0.00229352]\\\n",
    "Cost: 0.6649331468590681\\\n",
    "------Epoch 2------\\\n",
    "Theta: [-5.93604956e-05  8.33145873e-03  1.07754324e-02]\\\n",
    "Cost: 0.6679914364992459\\\n",
    "------Epoch 3------\\\n",
    "Theta: [-0.0002356   0.0004607   0.00370829]\\\n",
    "Cost: 0.6545873034874964\\\n",
    "------Epoch 4------\\\n",
    "Theta: [-0.00020363  0.00683227  0.01065138]\\\n",
    "Cost: 0.6563302142684528\n",
    "4. You may take the plots below as an exmample: \n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1xLg9LrIF888gGXj3zRAG9iJLsyAmgPQg)\n",
    "\n",
    "5. It may take ~1 min to finish running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Implementation Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Decision Boundary Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.6 Evaluating Logistic Regression\n",
    "\n",
    "Evaluate the model with given data. **(15 Points)**\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate the training accuracy and **PRINT IT OUT**.\n",
    "2. Evaluate the predicted probability of the learnt model with x1 = 56 and x2 = 32 and **PRINT IT OUT**.\n",
    "\n",
    "\n",
    "**Hints:**  \n",
    "1. Positive(prediction>0.5) and negative(prediction<=0.5). \n",
    "2. The prediction results are based on acceptance probability. Given the two exam scores, we expected the model yields either high probability of \"fail\" or low probability of \"pass\".\n",
    "3. Training accuracy should be around **85%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
